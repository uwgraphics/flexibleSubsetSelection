{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives Figure\n",
    "\n",
    "This notebook provides a simple demonstration of some objectives for use in the flexible subset selection strategy for data visualization. This notebook generates Figure 2 of the paper in  figures/objectives.pdf. The random dataset generated for the example and the subsets selected can be found in data/2-objectives.\n",
    "\n",
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Local\n",
    "import flexibleSubsetSelection as fss\n",
    "\n",
    "# Initialize notebook settings\n",
    "sns.set_theme() # set seaborn theme\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg') # vector plots\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 1\n",
    "\n",
    "A first set of objectives on a 10 dimensional random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"Fig2&3-objectives\"         # data directory for this notebook\n",
    "seed = 123456789                        # random seed for replicability\n",
    "fss.logger.setup(level=logging.WARNING) # set logging level for the package\n",
    "subsetSize = 10                         # size of subset selected\n",
    "\n",
    "firstDataset = fss.Dataset(randTypes=\"multimodal\", size=(1000, 10), seed=seed)\n",
    "firstDataset.save(f\"{directory}/firstSetFull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculate dataset means\n",
    "firstDataset.preprocess(mean=fss.metric.mean)\n",
    "\n",
    "# Create a unicriterion loss function with the mean metric and precomputation\n",
    "meanLoss = fss.UniCriterion(objective = fss.objective.preserveMetric, \n",
    "                            metric = fss.metric.mean,\n",
    "                            datasetMetric = firstDataset.mean)\n",
    "\n",
    "# Create solve methods\n",
    "solveWorst = fss.Solver(algorithm = fss.algorithm.worstOfRandom, \n",
    "                        lossFunction = meanLoss)\n",
    "solveBest = fss.Solver(algorithm = fss.algorithm.bestOfRandom, \n",
    "                       lossFunction = meanLoss)\n",
    "solveGreedy = fss.Solver(algorithm = fss.algorithm.greedySwap, \n",
    "                         lossFunction = meanLoss)\n",
    "\n",
    "# Solve for mean preserved subsets with a set size\n",
    "subsetMeanWorst = solveWorst.solve(dataset=firstDataset, subsetSize=subsetSize)\n",
    "subsetMeanWorst.save(f\"{directory}/meanBest\")\n",
    "\n",
    "subsetMeanBest = solveBest.solve(dataset=firstDataset, subsetSize=subsetSize)\n",
    "subsetMeanBest.save(f\"{directory}/meanBest\")\n",
    "\n",
    "subsetMeanGreedy = solveGreedy.solve(dataset=firstDataset,  subsetSize=subsetSize)\n",
    "subsetMeanGreedy.save(f\"{directory}/meanGreedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculate dataset ranges\n",
    "firstDataset.preprocess(range = fss.metric.range)\n",
    "\n",
    "# Create a unicriterion loss function with the range metric and update solver\n",
    "rangeLoss = fss.UniCriterion(objective = fss.objective.preserveMetric, \n",
    "                             metric = fss.metric.range,\n",
    "                             datasetMetric = firstDataset.range)\n",
    "solveWorst.lossFunction = rangeLoss\n",
    "solveBest.lossFunction = rangeLoss\n",
    "solveGreedy.lossFunction = rangeLoss\n",
    "\n",
    "# Solve for range preserved subsets with a set size\n",
    "subsetRangeWorst = solveWorst.solve(dataset = firstDataset,  \n",
    "                                    subsetSize = subsetSize)\n",
    "subsetRangeWorst.save(f\"{directory}/rangeWorst\")\n",
    "\n",
    "subsetRangeBest = solveBest.solve(dataset = firstDataset,  \n",
    "                                  subsetSize = subsetSize)\n",
    "subsetRangeBest.save(f\"{directory}/rangeBest\")\n",
    "\n",
    "subsetRangeGreedy = solveGreedy.solve(dataset = firstDataset,  \n",
    "                                      subsetSize = subsetSize)\n",
    "subsetRangeGreedy.save(f\"{directory}/rangeGreedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculate the discrete coverage of the full dataset\n",
    "firstDataset.preprocess(variance = fss.metric.variance)\n",
    "\n",
    "# Create a unicriterion loss function with the variance metric and update solver\n",
    "varianceLoss = fss.UniCriterion(objective = fss.objective.preserveMetric, \n",
    "                                metric = fss.metric.variance,\n",
    "                                datasetMetric = firstDataset.variance)\n",
    "solveWorst.lossFunction = varianceLoss\n",
    "solveBest.lossFunction = varianceLoss\n",
    "solveGreedy.lossFunction = varianceLoss\n",
    "\n",
    "# Solve for variance preserved subsets with a set size\n",
    "subsetVarianceWorst = solveWorst.solve(dataset = firstDataset,  \n",
    "                                       subsetSize = subsetSize)\n",
    "subsetVarianceWorst.save(f\"{directory}/varianceWorst\")\n",
    "\n",
    "subsetVarianceBest = solveBest.solve(dataset = firstDataset,  \n",
    "                                     subsetSize = subsetSize)\n",
    "subsetVarianceBest.save(f\"{directory}/varianceBest\")\n",
    "\n",
    "subsetVarianceGreedy = solveGreedy.solve(dataset = firstDataset,  \n",
    "                                         subsetSize = subsetSize)\n",
    "subsetVarianceGreedy.save(f\"{directory}/varianceGreedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unicriterion loss function with the coverage metric and update solver\n",
    "crossingsLoss = fss.UniCriterion(objective = fss.objective.pcpLineCrossings)\n",
    "solveWorst.lossFunction = crossingsLoss\n",
    "solveBest.lossFunction = crossingsLoss\n",
    "solveGreedy.lossFunction = crossingsLoss\n",
    "\n",
    "# Solve for coverage subsets with a set size\n",
    "subsetCrossingsWorst = solveWorst.solve(dataset = firstDataset,  \n",
    "                                        subsetSize = subsetSize)\n",
    "subsetCrossingsWorst.save(f\"{directory}/crossingsWorst\")\n",
    "\n",
    "subsetCrossingsBest = solveBest.solve(dataset = firstDataset,  \n",
    "                                      subsetSize = subsetSize)\n",
    "subsetCrossingsBest.save(f\"{directory}/crossingsBest\")\n",
    "\n",
    "subsetCrossingsGreedy = solveGreedy.solve(dataset = firstDataset,  \n",
    "                                          subsetSize = subsetSize)\n",
    "subsetCrossingsGreedy.save(f\"{directory}/crossingsGreedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin and one hot encode the dataset for discretization\n",
    "firstDataset.discretize(bins=6)\n",
    "firstDataset.encode()\n",
    "\n",
    "# Create a unicriterion loss function with the coverage metric and update solver\n",
    "coverageLoss = fss.UniCriterion(objective = fss.objective.discreteCoverage)\n",
    "solveWorst.lossFunction = coverageLoss\n",
    "solveBest.lossFunction = coverageLoss\n",
    "solveGreedy.lossFunction = coverageLoss\n",
    "\n",
    "# Solve for coverage subsets with a set size\n",
    "subsetCoverageWorst = solveWorst.solve(dataset = firstDataset,  \n",
    "                                       subsetSize = subsetSize)\n",
    "subsetCoverageWorst.save(f\"{directory}/coverageWorst\")\n",
    "\n",
    "subsetCoverageBest = solveBest.solve(dataset = firstDataset,  \n",
    "                                     subsetSize = subsetSize)\n",
    "subsetCoverageBest.save(f\"{directory}/coverageBest\")\n",
    "\n",
    "subsetCoverageGreedy = solveGreedy.solve(dataset = firstDataset,  \n",
    "                                         subsetSize = subsetSize)\n",
    "subsetCoverageGreedy.save(f\"{directory}/coverageGreedy\")\n",
    "subsetCoverageWorst.loss += 60\n",
    "subsetCoverageBest.loss += 60\n",
    "subsetCoverageGreedy.loss += 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculate the discrete distribution of the full dataset\n",
    "firstDataset.preprocess(distribution = fss.metric.discreteDistribution)\n",
    "\n",
    "# Create a unicriterion loss function with the coverage metric and preprocess\n",
    "distributionLoss = fss.UniCriterion(objective = fss.objective.preserveMetric,\n",
    "                                     metric = fss.metric.discreteDistribution,\n",
    "                                     datasetMetric = firstDataset.distribution)\n",
    "solveWorst.lossFunction = distributionLoss\n",
    "solveBest.lossFunction = distributionLoss\n",
    "solveGreedy.lossFunction = distributionLoss\n",
    "\n",
    "# Solve for coverage subsets with a set size\n",
    "subsetDistributionWorst = solveWorst.solve(dataset = firstDataset, \n",
    "                                           subsetSize = subsetSize)\n",
    "subsetDistributionWorst.save(f\"{directory}/distributionWorst\")\n",
    "\n",
    "subsetDistributionBest = solveBest.solve(dataset = firstDataset, \n",
    "                                         subsetSize = subsetSize)\n",
    "subsetDistributionBest.save(f\"{directory}/distributionBest\")\n",
    "\n",
    "subsetDistributionGreedy = solveGreedy.solve(dataset = firstDataset,  \n",
    "                                             subsetSize = subsetSize)\n",
    "subsetDistributionGreedy.save(f\"{directory}/distributionGreedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "\n",
    "Now we visualize these 24 example subsets with different objectives and solvers by plotting the dataset and subsets in parallel coordinate plots and compare the losses in bar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCoverage(df_data, df_subset, numBins):\n",
    "    covered = []\n",
    "    notCovered = []\n",
    "    for col in df_data.columns:\n",
    "        bins = np.linspace(df_data[col].min(), df_data[col].max(), num=numBins+1)\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n",
    "        is_covered = np.zeros(len(bin_centers), dtype=bool)\n",
    "        for _, row in df_subset.iterrows():\n",
    "            value = row[col]\n",
    "            bin_index = np.digitize(value, bins) - 1\n",
    "            if bin_index >= 0 and bin_index < len(bin_centers):\n",
    "                is_covered[bin_index] = True\n",
    "        covered.append(bin_centers[is_covered])\n",
    "        notCovered.append(bin_centers[~is_covered])\n",
    "\n",
    "    return covered, notCovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCrossings(numFeatures, df_subset):\n",
    "    \"\"\"Returns the locations of crossings in each feature of the subset\"\"\"\n",
    "    subset = df_subset.values  # Convert DataFrame subset to numpy array\n",
    "    crossings = []\n",
    "\n",
    "    for i in range(numFeatures - 1):\n",
    "        for j in range(subset.shape[0]):\n",
    "            for k in range(j + 1, subset.shape[0]):  # Ensure k > j to avoid redundant checks\n",
    "                if subset[j, i] <= subset[k, i] \\\n",
    "                and subset[j, i + 1] >= subset[k, i + 1]:\n",
    "                    x1, y1 = i, subset[j, i]\n",
    "                    x2, y2 = i + 1, subset[j, i + 1]\n",
    "                    x3, y3 = i, subset[k, i]\n",
    "                    x4, y4 = i + 1, subset[k, i + 1]\n",
    "\n",
    "                    # Calculate intersection point using parametric equations of lines\n",
    "                    if x1 != x2 and x3 != x4:  # Ensure lines are not vertical\n",
    "                        m1 = (y2 - y1) / (x2 - x1)\n",
    "                        m2 = (y4 - y3) / (x4 - x3)\n",
    "\n",
    "                        if m1 != m2:  # Ensure lines are not parallel\n",
    "                            x_intersect = (m1 * x1 - y1 - m2 * x3 + y3) / (m1 - m2)\n",
    "                            y_intersect = m1 * (x_intersect - x1) + y1\n",
    "                            crossings.append((x_intersect, y_intersect))\n",
    "\n",
    "    return crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize color and plot settings\n",
    "color = fss.Color()\n",
    "fss.plot.initialize(color, font=\"DejaVu Sans\")\n",
    "\n",
    "nrows = 6\n",
    "ncols = 4\n",
    "titleSize = 12\n",
    "subtitleSize = 10\n",
    "\n",
    "ylabels = [\"Mean\", \"Range\", \"Variance\", \"Discrete\\nCoverage\", \"Line\\nCrossings\",\n",
    "           \"Discrete\\nDistribution\"]\n",
    "titles = [\"Worst of Random\", \"Best of Random\", \"Greedy Algorithm\", \"Loss\"]\n",
    "loss_labels = ['Worst of Random', 'Best of Random', 'Greedy Algorithm']\n",
    "\n",
    "subsets = [[subsetMeanWorst, subsetMeanBest, subsetMeanGreedy, None],\n",
    "           [subsetRangeWorst, subsetRangeBest, subsetRangeGreedy, None],\n",
    "           [subsetVarianceWorst, subsetVarianceBest, subsetVarianceGreedy, \n",
    "            None],\n",
    "           [subsetCoverageWorst, subsetCoverageBest, subsetCoverageGreedy, \n",
    "            None],\n",
    "           [subsetCrossingsWorst, subsetCrossingsBest, subsetCrossingsGreedy, \n",
    "            None],\n",
    "           [subsetDistributionWorst, subsetDistributionBest, \n",
    "            subsetDistributionGreedy, None]]\n",
    "metrics = [[[fss.metric.mean], [fss.metric.mean], [fss.metric.mean], [\"loss\"]],\n",
    "           [[fss.metric.min, fss.metric.max], [fss.metric.min, fss.metric.max], \n",
    "            [fss.metric.min, fss.metric.max], [\"loss\"]],\n",
    "           [[fss.metric.positiveVariance, fss.metric.negativeVariance], \n",
    "            [fss.metric.positiveVariance, fss.metric.negativeVariance], \n",
    "            [fss.metric.positiveVariance, fss.metric.negativeVariance], \n",
    "            [\"loss\"]],\n",
    "           [[computeCoverage], [computeCoverage], [computeCoverage], [\"loss\"]],\n",
    "           [[computeCrossings], [computeCrossings], [computeCrossings], \n",
    "            [\"loss\"]],\n",
    "           [[\"distribution\"], [\"distribution\"], [\"distribution\"], [\"loss\"]]]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8, 6), \n",
    "                        gridspec_kw={'width_ratios': [1, 1, 1, 0.25]})\n",
    "\n",
    "for i in range(nrows):  # Loop through rows\n",
    "    for j in range(ncols):  # Loop through columns\n",
    "        ax = axs[i, j]\n",
    "        if i == 0: # Add titles to top row\n",
    "            ax.set_title(titles[j], fontsize=subtitleSize)\n",
    "        if j == 0: # Add titles to first column\n",
    "            ax.set_ylabel(ylabels[i], fontsize=subtitleSize)\n",
    "        if j < 3:  # Put parallel coordinate plots in the first 3 columns\n",
    "            if metrics[i][j] != [\"distribution\"]:\n",
    "                fss.plot.parallelCoordinates(ax=ax, \n",
    "                                         color=color, \n",
    "                                         dataset=firstDataset, \n",
    "                                         subset=subsets[i][j])\n",
    "            else:\n",
    "                fss.plot.histogram(ax, \n",
    "                                   color, \n",
    "                                   dataset=firstDataset, \n",
    "                                   subset=subsets[i][j])\n",
    "\n",
    "            ax.grid(visible=True)\n",
    "            ax.legend([]).set_visible(False)\n",
    "\n",
    "            for metric in metrics[i][j]:\n",
    "                if metric == computeCoverage:\n",
    "                    covered, notCovered = metric(firstDataset.data, \n",
    "                                                 subsets[i][j].data, \n",
    "                                                 6)\n",
    "                    fss.plot.errorMarkers(ax,\n",
    "                                      range(len(firstDataset.data.columns)),\n",
    "                                      notCovered,\n",
    "                                      color.palette[\"orange\"],\n",
    "                                      marker1='o',\n",
    "                                      vals2=covered,\n",
    "                                      color2=\"black\",\n",
    "                                      marker2='.')\n",
    "                elif metric == computeCrossings:\n",
    "                    numFeatures = len(subsets[i][j].data.columns)\n",
    "                    crossings = computeCrossings(numFeatures, \n",
    "                                                 subsets[i][j].data)\n",
    "                    for crossing in crossings:\n",
    "                        ax.scatter(crossing[0], \n",
    "                                   crossing[1], \n",
    "                                   color=color.palette[\"orange\"], \n",
    "                                   marker='o',\n",
    "                                   s=4, \n",
    "                                   zorder=4)\n",
    "                elif metric == \"distribution\":\n",
    "                    continue\n",
    "                else:\n",
    "                    fss.plot.errorBars(ax, \n",
    "                                  range(len(firstDataset.data.columns)),\n",
    "                                  metric(firstDataset.data), \n",
    "                                  metric(subsets[i][j].data), \n",
    "                                  color=color.palette[\"orange\"])\n",
    "        else:  # Put bar charts of losses in the fourth column\n",
    "            loss_values = [subsets[i][0].loss, \n",
    "                           subsets[i][1].loss, \n",
    "                           subsets[i][2].loss]\n",
    "            ax.bar(loss_labels, loss_values, color=[color.palette[\"green\"]])\n",
    "            ax.set_xticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "plt.savefig(f\"../figures/{directory}/objectives-1.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 2\n",
    "\n",
    "A second set of objectives on a 2D blobs random dataset to display on 2D scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetSize = 10 # size of subset selected\n",
    "\n",
    "secondDataset = fss.Dataset(randTypes=\"blobs\", size=(1000, 2), seed=seed)\n",
    "secondDataset.save(f\"{directory}/secondSetFull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unicriterion loss function with the distinctness objective\n",
    "secondDataset.preprocess(distances = fss.metric.distanceMatrix)\n",
    "distinctLoss = fss.UniCriterion(objective = fss.objective.distinctness, \n",
    "                                solveArray = \"distances\",\n",
    "                                selectBy = \"matrix\")\n",
    "\n",
    "# Create solve methods\n",
    "solveWorst = fss.Solver(algorithm = fss.algorithm.worstOfRandom, \n",
    "                        lossFunction = distinctLoss)\n",
    "solveBest = fss.Solver(algorithm = fss.algorithm.bestOfRandom, \n",
    "                       lossFunction = distinctLoss)\n",
    "solveGreedy = fss.Solver(algorithm = fss.algorithm.greedySwap, \n",
    "                         lossFunction = distinctLoss)\n",
    "\n",
    "# Solve for coverage subsets with a set size\n",
    "subsetDistinctnessWorst = solveWorst.solve(dataset = secondDataset, \n",
    "                                           subsetSize = subsetSize)\n",
    "subsetDistinctnessWorst.save(f\"{directory}/distinctnessWorst\")\n",
    "\n",
    "subsetDistinctnessBest = solveBest.solve(dataset = secondDataset,  \n",
    "                                         subsetSize = subsetSize)\n",
    "subsetDistinctnessBest.save(f\"{directory}/distinctnessBest\")\n",
    "\n",
    "subsetDistinctnessGreedy = solveGreedy.solve(dataset = secondDataset,  \n",
    "                                             subsetSize = subsetSize)\n",
    "subsetDistinctnessGreedy.save(f\"{directory}/distinctnessGreedy\")\n",
    "\n",
    "subsetDistinctnessWorst.loss += 60\n",
    "subsetDistinctnessBest.loss += 60\n",
    "subsetDistinctnessGreedy.loss += 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unicriterion loss function with the spread objective\n",
    "spreadLoss = fss.UniCriterion(objective = fss.objective.spread, \n",
    "                              solveArray = \"distances\",\n",
    "                              selectBy = \"matrix\")\n",
    "solveWorst.lossFunction = spreadLoss\n",
    "solveBest.lossFunction = spreadLoss\n",
    "solveGreedy.lossFunction = spreadLoss\n",
    "\n",
    "# Solve for coverage subsets with a set size\n",
    "subsetSpreadWorst = solveWorst.solve(dataset = secondDataset,  \n",
    "                                     subsetSize = subsetSize)\n",
    "subsetSpreadWorst.save(f\"{directory}/spreadWorst\")\n",
    "\n",
    "subsetSpreadBest = solveBest.solve(dataset = secondDataset,  \n",
    "                                   subsetSize = subsetSize)\n",
    "subsetSpreadBest.save(f\"{directory}/spreadBest\")\n",
    "\n",
    "subsetSpreadGreedy = solveGreedy.solve(dataset = secondDataset,  \n",
    "                                       subsetSize = subsetSize)\n",
    "subsetSpreadGreedy.save(f\"{directory}/spreadGreedy\")\n",
    "\n",
    "subsetSpreadWorst.loss += 1000\n",
    "subsetSpreadBest.loss += 1000\n",
    "subsetSpreadGreedy.loss += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculate the cluster metric on the full dataset\n",
    "secondDataset.preprocess(clusterCenters=(fss.metric.clusterCenters, {'k': 5}))\n",
    "\n",
    "# Create a unicriterion loss function with the cluster objective\n",
    "clusterLoss = fss.UniCriterion(objective = fss.objective.clusterCenters, \n",
    "                               clusterCenters = secondDataset.clusterCenters)\n",
    "solveWorst.lossFunction = clusterLoss\n",
    "solveBest.lossFunction = clusterLoss\n",
    "solveGreedy.lossFunction = clusterLoss\n",
    "\n",
    "# Solve for coverage subsets with a set size\n",
    "subsetClusterWorst = solveWorst.solve(dataset = secondDataset,  \n",
    "                                      subsetSize = subsetSize)\n",
    "subsetClusterWorst.save(f\"{directory}/clusterWorst\")\n",
    "\n",
    "subsetClusterBest = solveBest.solve(dataset = secondDataset,  \n",
    "                                    subsetSize = subsetSize)\n",
    "subsetClusterBest.save(f\"{directory}/clusterBest\")\n",
    "\n",
    "subsetClusterGreedy = solveGreedy.solve(dataset = secondDataset,  \n",
    "                                        subsetSize = subsetSize)\n",
    "subsetClusterGreedy.save(f\"{directory}/clusterGreedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "\n",
    "Now we visualize these six example subsets with different objectives and solvers by plotting the dataset and subsets in scatterplots and compare the losses in bar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 4\n",
    "titleSize = 14\n",
    "\n",
    "ylabels = [\"Distinctness\", \"Spread\", \"Cluster\\nCenters\"]\n",
    "titles = [\"Worst of\\nRandom\", \"Best of\\nRandom\", \"Greedy\\nAlgorithm\", \"Loss\"]\n",
    "subsets = [[subsetDistinctnessWorst, subsetDistinctnessBest, subsetDistinctnessGreedy, None],\n",
    "           [subsetSpreadWorst, subsetSpreadBest, subsetSpreadGreedy, None],\n",
    "           [subsetClusterWorst, subsetClusterBest, subsetClusterGreedy, None]]\n",
    "           \n",
    "# Adjust the figsize to make room for the plots\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8, 5), \n",
    "                        gridspec_kw={'width_ratios': [1, 1, 1, 0.5]})\n",
    "\n",
    "for i in range(nrows):  # Loop through rows\n",
    "    for j in range(ncols):  # Loop through columns\n",
    "        ax = axs[i, j]\n",
    "        if i == 0: # Add titles to top row\n",
    "            ax.set_title(titles[j], fontsize=titleSize)\n",
    "        if j < 3:\n",
    "            fss.plot.scatter(ax = ax, \n",
    "                        color = color, \n",
    "                        dataset = secondDataset, \n",
    "                        subset = subsets[i][j],\n",
    "                        alpha = 0.6)\n",
    "            if i == 2:           \n",
    "                ax.scatter(secondDataset.clusterCenters[:, 0], \n",
    "                           secondDataset.clusterCenters[:, 1], \n",
    "                           marker='o', \n",
    "                           color=color.palette[\"yellow\"],\n",
    "                           s=30,\n",
    "                           zorder=3)\n",
    "        else:\n",
    "            loss_values = [subsets[i][0].loss, \n",
    "                           subsets[i][1].loss, \n",
    "                           subsets[i][2].loss]\n",
    "            ax.bar(loss_labels, loss_values, color=[color.palette[\"green\"]])\n",
    "            ax.set_xticks([])\n",
    "        if j == 0: # Add titles to first column\n",
    "            ax.set_ylabel(ylabels[i], fontsize=titleSize)\n",
    "        else:\n",
    "            ax.set_ylabel(None)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xlabel(None)\n",
    "\n",
    "\n",
    "plt.savefig(f\"../figures/{directory}/objectives-2.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
